As a **systems researcher** in **Human-Computer Interaction**, I embed machine learning technologies (*e.g.*, Natural Language Processing) into new human interactions that I then deploy to test. Using my systems, remote [content creators][1] more effectively collaborate, [video authors][2] efficiently create accessible descriptions for blind users, and [instructors][3] help students to learn and retain key points. To inform future systems that capture what is important to domain experts and people with disabilities, I also conduct and collaborate on in-depth qualitative (*e.g.*, [AAC communication][4], [memes][5]) and quantitative studies (*e.g.*, [360Â° Video][6], [VR Saliency][7]). My long term research goal is to make communication more effective and accessible to all.

[1]: https://dl.acm.org/doi/10.1145/2984511.2984552
[2]: https://dl.acm.org/doi/10.1145/3379337.3415864
[3]: https://videodigests.com/
[4]: https://dl.acm.org/doi/10.1145/3313831.3376376
[5]: https://dl.acm.org/doi/10.1145/3308561.3353792
[6]: https://aksp.github.io/interactive360video/
[7]: https://vsitzmann.github.io/vr-saliency/
